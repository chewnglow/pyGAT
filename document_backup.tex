\documentclass[onecolumn, conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[noend]{algpseudocode}
\usepackage{algorithmicx,algorithm}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Conference Paper Title*\\
{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
should not be used}
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
%\and
%\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and\usepackage[noend]{algpseudocode}
%\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
}

\maketitle

%\begin{abstract}
%This document is a model and instructions for \LaTeX.
%This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
%or Math in Paper Title or Abstract.
%\end{abstract}

%\begin{IEEEkeywords}
%component, formatting, style, styling, insert
%\end{IEEEkeywords}

\section{Introduction}
\subsection{Current work}
Currently typical message passing mechanisms of graph neural networks (GNN) are highly related to the graph structure. For example, graph attention networks (GAT)\cite{gat} passes information via attention sharing on global nodes, graph convolution network (GCN)\cite{gcn} performs approximations on spectral convolution. However, there still exists information which are not position-aware, which indicates they are unable to be shared during the position aware message passing. For example, in relation networks like social networks and citation networks, there may exists orphan nodes which contains features not similar to those of connected nodes. Typical solution is random sampling on the whole node set of the graph, regardless of the edge connections. Thus successful extraction of these structure-independent features may significantly boosts the representation performance of the node. Typical solution is random sampling on the whole node set of the graph, regardless of the edge connections. For example, P-GNN\cite{b1} randomly samples several subsets containing arbitrary count of nodes on the original graph, and stacks as a  feature matrix. All subsets further aggregate into a shared vector by aggregation (subset-wise mean), and fused to each node feature vector. While GraphSTONE\cite{b2} divides the graph via random walk and find shared information from different samples on the unique structure topic. However, random sampling cannot perfectly extract the global information, which performance may influenced by the randomly sampled result. Mechanics with direct operation on the whole graph is required. 

Non-negative matrix factorization (NMF) is a matrix factorization mechanism which is able to learn parts-based representations by 2 low rank matrices, as the linear combination of the data-based basis and weights \cite{b3}, like other typical factorization methods like principle component analysis (PCA) and vector quantization (VQ). Among which, eigen-based methods PCA and VQ mainly extracts holistic features, where basis components manifesting building blocks of the original matrix may missing \cite{b4}. 
NMF makes the computation more efficient and has achieved wide application in engineering, such as data mining and image processing, to name a few. In image processing, \cite{b3} and \cite{b4} uses NMF to calculate the basis representation of human face data in facial recognition and classification tasks. In data mining, NMF is able to calculate the semantic feature representations in articles\cite{b3}. NMF has also become a basic baseline algorithm in recommendation system as representing the individual-item relation by refined individual-topic and topic-item relations. Besides, GraphSTONE\cite{b2} also uses NMF to factorize the key structure topic of sampled results from different random walks, and lead to the following principal component calculation. However typically, NMF is applied in end to end approaches as feature extractor,  where potential distribution shift may exists between the learned representation by NMF and the original node features. Thus separated implement of NMF may still cause performance decreasing in contrast with the approach without NMF. 

\subsection{Our contribution}
We purpose a novel approach enhancing the message passing of GNNs via non-negative matrix factorization (NMF), for leading the structure-invariant components into the node features. Inspired by above researches, we directly perform NMF on the feature matrix and gained a global basis matrix and the corresponding weight matrix. The operation on the whole matrix imposes the global feature distribution into the factorized basis, which is further embedded into a feature vector and send into message passing together with original node features. 
Furthermore, to better integrate NMF into message passing framework,  the basis matrix is randomly initialized and update via backward propagation with the whole graph rather than explicitly perform NMF on the original feature matrix, to achieve distribution consistency between the feature of basis representation and node features. 

\section{Methods}
The general pipeline of our approach is shown in figure \ref{label}.
The original matrix of node features $ \mathbf{A}\in \mathbb{R}^{n\times m} $ is factorized as $ \bf A\approx WH $, where $ \mathbf{W}\in \mathbb{R}^{n\times k} $ and $ \mathbf{H}\in \mathbb{R}^{k\times m} $ represents the basis and combination matrix respectively, where $ k $ is manually defined. Then basis matrix $ \mathbf{W} $ is embedded into a feature vector $ \bf w $ by aggregation operations. In our implementation the aggregation is chosen as row-wise mean, which is $  \mathbf{w}=\frac{1}{N}\sum\limits_{i=1}^N \mathbf{W}_{i, :} $. These operation are all differentiable which can be optimized via backpropagation. 
Then $ \bf w $ is concatenated with all node features $ \mathbf{v}_i,\. i\in [1,N] $ into the new vector $ \mathbf{v}'_i $, which is denoted as 
\begin{equation}\label{key}
	\mathbf{v'}_i = \Bigg|\Bigg|\left\{\mathbf{v}_i, \mathbf{w}\right\}.
\end{equation}
Then the fused vectors are taken into the following message passing defined by GAT. GAT calculates self-attention inside the node set, which attention score calculated between linear mapped key vectors $ \mathbf{k}_i $ and query vectors $ \mathbf{q}_j $ in the node set, where the linear mapping matrix $ \bf W $ is shared in the message passing. The process can be mathematically expressed as
\begin{equation}\label{key}
	\mathbf{A} = \delta(\frac{\mathbf{W}\mathbf{Q} \mathbf{K}^\intercal\mathbf{W}^\intercal}{\sqrt{\sigma}})
\end{equation}
where $ \mathbf{A}_{ij} $ denotes the attention score between $ i $-th query vector and $ j $-th key vector. 


\subsection{NMF}
NMF is a matrix factorization method wildly used in recommend systems and text mining. In the text mining task, we use this method to analyze text topics. For any text sets with $n$ texts and $m$ words, given an nonnegative matrix $X\in\mathbb{R}^{n\times m}$, it can find a nonnegative matrix $W\in\mathbb{R}^{n\times t}$ to represent the relevance of each text to $t$ topics, and a nonnegative matrix $H\in\mathbb{R}^{t \times m}$ to show how the topics relate to the words, which satisfies the condition $X=WH$, and decomposes a nonnegative matrix into the product of left and right nonnegative matrices. The text $X$ is restructured by $W$ and $H$, consequently, the goal is $min||X-WH||^2$. The update rule for NMF algorithm is in Algorithm 1 according to \cite{NIPS2000_f9d11525}.

\begin{algorithm}[t]
	\caption{NMF} %算法的名字
	\hspace*{0.02in} { Input: $X$, max iteration number $iter$} %算法的输入， \hspace*{0.02in}用来控制位置，同时利用 \\ 进行换行
	\hspace*{0.02in} {\bf Output: $W,H$} %算法的结果输出
	\label{NMF}
	\begin{algorithmic}[1]
		\State random initial $W$ matrix and $H$ matrix % \State 后写一般语句
		\For{$i$ in $iter$} % For 语句，需要和EndFor对应
		\State  $H'\leftarrow H \frac{W^TX}{W^TWH}$
		\State $W'\leftarrow W\frac{XH^T}{WHH^T}$
		\State $H\leftarrow H'$
		\State $W\leftarrow W'$
		\EndFor
		\Return $W,H$
	\end{algorithmic}
\end{algorithm}
\subsection{Spectural Graph}
Graph neural network is a method of connecting information between samples. This method can predict the information of neighbor nodes according to the properties of the points. Among them, most graph networks are based on spectral graph theory. A graph is defined as $G=(X,E)$, where $X=\{\bf x_1,x_2,\dots,x_n\}$ is the node set and $\bf E\in\mathbb{R}^{n\times n}$ represent whether exist the edges in the graph. To simulation the fusion on the graph, we use the Laplacian Matrix which is $\bf L=D-E=dI-E=U\Lambda U^T$, where $\bf U$ is eigenvector matrix and $\bf \Lambda$ is the eigenvalue diagonal matrix. According to the definition of the matrix, the motion direction of the vector can be changed by the matrix $A$. For each vector in the graph, the transition is $\bf \lambda x =A x$.\cite{2} In the Laplacian transmition,  $A\in\mathbb{R}^{n\times n}$ is an approximate matrix based on Chebyshev Theory and $\theta\in\mathbb{R}^{n\times n}$. Thus, the transition is
\[
\bf g_{\theta}x=Ug_{\theta}U^Tx=Ug_{\theta}(\Lambda)U^Tx=Ax,
\]
where $\bf g_{\theta}$ is the a diagonal matrix with Chebyshev characteristic of $\lambda$, and $\bf g_{\theta}=g_{\theta}(\Lambda)$\cite{3}. Thus, the Laplasian transition is in the Chebyshev form where $\bf \tilde{L}=U\tilde{L}U^T, \tilde{L}=A$ and since the $\bf g_{\theta}x=Ax=\tilde{L}x$. In graph convolution neural network\cite{4}, the transition is approximate to,
\[
\bf g_{\theta}x=\theta_0x-\theta_1D^{-1/2}ED^{-1/2}x.
\]
For simplicity, supposing $\theta=\theta_0=-\theta_1$, the form is,
\[
\bf g_{\theta}x=\theta(I+1D^{-1/2}ED^{-1/2})x.
\]
After the normalization,
\[
\bf \hat{X}=\tilde{D}^{-1/2}E\tilde{D}^{-1/2}\Theta,
\]
where $\bf \Theta\in\mathbb{R}^{n\times m}$.

As shown below, the GCN describ the information transition based on the edges. Similarly, the graph attention network (GAT) is in the form of $\bf \hat{X}=EW_aX$, where $\bf W_a$ is the weight matrix from attention coefficient\cite{4}. Both GCN and GAT defines the information transition through the edges with the information from neigbor nodes. However, the labels also depend on the similar contents among the whole dataset.

%Non-negative matrix factorization (NMF) is a matrix approximation mechanism of representing the big original matrix $ A $ by the product of 2 low rank matrices as $ \bf A\approx WH $. Among which, the factorization can be represented as expressing the original matrix by a set of more related basis, where $ \bf W $ is the basis matrix and $ \mathbf{H} $ is the projection to those basis.

%\section{Results}
%We've implemented a naive run on Cora dataset\cite{cora}, which consists 2708 scientific publication instances classified into 7 classes. The dataset provides a publication-keywords-classification characteristic table and a publication-publication citation relationship table. The implementation uses mean method to aggregate basis matrix into global feature vector, and uses GAT as the message passing backbone. The experiment result is shown in table \ref{result}, where the model is run on single NVIDIA GTX 1080 Ti GPU. Current implement has become a effective approximate of original GAT, while still exists optimization. 
%
%\begin{table}[htbp]
%	\caption{Performances of our implementation compared to original GAT, measured in ROC AUC.}
%	\begin{center}
%	\begin{tabular}{|c|c|c|}
%		\hline
%		Implements & Inference Accuracy & Time consultant on each epoch (s) \\
%		\hline
%		GAT& 0.8453 & 0.912s \\
%		\hline
%		Ours& 0.8167 & 1.108s \\
%		\hline
%	\end{tabular}
%	\label{result}
%	\end{center}
%\end{table}

%\section*{Acknowledgment}
%
%The preferred spelling of the word ``acknowledgment'' in America is without 
%an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
%G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
%acknowledgments in the unnumbered footnote on the first page.

%\section*{References}

%Please number citations consecutively within brackets \cite{b1}. The 
%sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
%number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
%the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''
%
%Number footnotes separately in superscripts. Place the actual footnote at 
%the bottom of the column in which it was cited. Do not put footnotes in the 
%abstract or reference list. Use letters for table footnotes.
%
%Unless there are six authors or more give all authors' names; do not use 
%``et al.''. Papers that have not been published, even if they have been 
%submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
%that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
%Capitalize only the first word in a paper title, except for proper nouns and 
%element symbols.
%
%For papers published in translation journals, please give the English 
%citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}
\bibitem{b1} You, J., Ying, R., \& Leskovec, J. (2019). Position-aware graph neural networks. arXiv preprint arXiv:1906.04817.
\bibitem{b2} Long, Q., Jin, Y., Song, G., Li, Y., \& Lin, W. (2020, August). Graph Structural-topic Neural Network. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining (pp. 1065-1073).
\bibitem{b3} Lee, D. D., \& Seung, H. S. (1999). Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755), 788-791.
\bibitem{b4} Feng, T., Li, S. Z., Shum, H. Y., \& Zhang, H. (2002, June). Local non-negative matrix factorization as a visual representation. In Proceedings 2nd International Conference on Development and Learning. ICDL 2002 (pp. 178-183). IEEE.
\bibitem{b5} Huang, Z., Zhou, A., \& Zhang, G. (2012, October). Non-negative matrix factorization: A short survey on methods and applications. In International Symposium on Intelligence Computation and Applications (pp. 331-340). Springer, Berlin, Heidelberg.
\bibitem{cora} Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., \& Eliassi-Rad, T. (2008). Collective classification in network data. AI magazine, 29(3), 93-93.
\bibitem{gat} Veličković, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., \& Bengio, Y. (2017). Graph attention networks. arXiv preprint arXiv:1710.10903.
\bibitem{gcn} Kipf, T. N., \& Welling, M. (2016). Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907.
\end{thebibliography}
%\vspace{12pt}
%\color{red}

\end{document}
